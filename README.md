# distributed-llama2-server
Deploy llama2 serving on multiple GPUs via flask
